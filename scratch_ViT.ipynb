{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T # to resize input images and convert to tensor, make image size divisible by patch size\n",
    "from torch.optim import Adam\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import numpy as np # for positional encodings using sine and cosine operation\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, model_dim, img_size, patch_size, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.linear_project = nn.Conv2d(\n",
    "            self.num_channels, \n",
    "            self.model_dim, \n",
    "            kernel_size=self.patch_size, \n",
    "            stride=self.patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_project(x) # (B, C, H, W) --> (B, model_dim, P_Row, P_Col)\n",
    "        x = x.flatten(2) # (B, model_dim, P_Row, P_Col) --> (B, model_dim, P)\n",
    "        x = x.transpose(1,2) # (B, model_dim, P) --> (B, P, model_dim)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Token and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_seq_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, model_dim)) # classification token\n",
    "\n",
    "        # positional encoding\n",
    "        pe = torch.zeros(max_seq_length, model_dim)\n",
    "\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(model_dim):\n",
    "                if i % 2 == 0:\n",
    "                    pe[pos][i] = np.sin(pos/(10000 ** (i/model_dim)))\n",
    "                else:\n",
    "                    pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/model_dim)))\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens_batch = self.cls_token.expand(x.size()[0], -1, -1) # class token for each image\n",
    "        x = torch.cat((tokens_batch,x), dim=1) # class token + no. of patches for each image\n",
    "        x = x + self.pe\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, model_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.query = nn.Linear(model_dim, head_size)\n",
    "        self.key = nn.Linear(model_dim, head_size)\n",
    "        self.value = nn.Linear(model_dim, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        attention = Q @ K.transpose(-2,-1)\n",
    "        attention = attention / (self.head_size ** 0.5) # scale value to control variance at initialization\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        attention = attention @ V\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = model_dim // num_heads\n",
    "        self.W_o = nn.Linear(model_dim, model_dim)\n",
    "        self.heads = nn.ModuleList([AttentionHead(model_dim, self.head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(model_dim)\n",
    "        self.mha = MultiHeadAttention(model_dim, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(model_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim * r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(model_dim * r_mlp, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # residual connections to prevent vanishing gradient problem\n",
    "        out = x + self.mha(self.ln1(x))\n",
    "        out = out + self.mlp(self.ln2(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, model_dim, num_classes, img_size, patch_size, num_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimension must be divisible by patch_size dimensions\"\n",
    "        assert model_dim % num_heads == 0, \"model_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.num_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "        self.max_seq_length = self.num_patches + 1\n",
    "        self.patch_embedding = PatchEmbedding(self.model_dim, self.img_size, self.patch_size, self.num_channels)\n",
    "        self.positional_encoding = PositionalEncoding(self.model_dim, self.max_seq_length)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoder(self.model_dim, self.num_heads) for _ in range(num_layers)])\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Linear(self.model_dim, self.num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.patch_embedding(images)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifer(x[:, 0])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 9\n",
    "num_classes = 10\n",
    "img_size = (32, 32)\n",
    "patch_size = (16, 16)\n",
    "num_channels = 1\n",
    "num_heads = 3\n",
    "num_layers = 3\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "alpha = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = T.Compose([\n",
    "#     T.Resize(img_size),\n",
    "#     T.ToTensor()\n",
    "# ])\n",
    "\n",
    "# train_set = MNIST(\n",
    "#     root='./../datasets', train= True, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "# test_set = MNIST(\n",
    "#     root='./../datasets', train=False, download=True, transform=transform\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = MNIST(\n",
    "    root='./../datasets', train= True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_set = MNIST(\n",
    "    root='./../datasets', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_indices, valid_indices = train_test_split(list(range(len(train_set))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_split = Subset(train_set, train_indices)\n",
    "valid_split = Subset(train_set, valid_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_split, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_split, shuffle=False, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# transformer = VisionTransformer(model_dim, num_classes, img_size, patch_size, num_channels, num_heads, num_layers).to(device)\n",
    "\n",
    "# optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "    \n",
    "#     training_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, data in enumerate(train_loader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = transformer(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         training_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}/{epochs} accuracy: {correct / total * 100:.3f} % loss: {training_loss / len(train_loader) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1/10 train accuracy: 54.612 % train loss: 1.917 valid accuracy: 71.875 % valid loss: 1.735\n",
      "Epoch 2/10 train accuracy: 75.935 % train loss: 1.704 valid accuracy: 80.469 % valid loss: 1.655\n",
      "Epoch 3/10 train accuracy: 77.892 % train loss: 1.683 valid accuracy: 80.469 % valid loss: 1.660\n",
      "Epoch 4/10 train accuracy: 85.173 % train loss: 1.611 valid accuracy: 88.281 % valid loss: 1.582\n",
      "Epoch 5/10 train accuracy: 87.740 % train loss: 1.584 valid accuracy: 85.156 % valid loss: 1.617\n",
      "Epoch 6/10 train accuracy: 89.223 % train loss: 1.569 valid accuracy: 91.406 % valid loss: 1.561\n",
      "Epoch 7/10 train accuracy: 90.062 % train loss: 1.561 valid accuracy: 89.844 % valid loss: 1.568\n",
      "Epoch 8/10 train accuracy: 90.460 % train loss: 1.557 valid accuracy: 89.844 % valid loss: 1.567\n",
      "Epoch 9/10 train accuracy: 90.135 % train loss: 1.560 valid accuracy: 91.406 % valid loss: 1.556\n",
      "Epoch 10/10 train accuracy: 90.417 % train loss: 1.557 valid accuracy: 89.844 % valid loss: 1.562\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "transformer = VisionTransformer(model_dim, num_classes, img_size, patch_size, num_channels, num_heads, num_layers).to(device)\n",
    "\n",
    "optimizer = Adam(transformer.parameters(), lr=alpha)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    transformer.train()\n",
    "    training_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = transformer(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        transformer.eval()\n",
    "        valid_loss = 0.0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = transformer(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_valid += labels.size(0)\n",
    "                correct_valid += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs} train accuracy: {correct / total * 100:.3f} % train loss: {training_loss / len(train_loader) :.3f} valid accuracy: {correct_valid / total_valid * 100:.3f} % valid loss: {valid_loss / len(valid_loader):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 90 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = transformer(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'\\nModel Accuracy: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "har",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
